{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7123f3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def greet(name, intensity):\n",
    "    return \"Hello, \" + name + \"!\" * int(intensity)\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=greet,\n",
    "    inputs=[\"text\", \"slider\"],\n",
    "    outputs=[\"text\"],\n",
    ")\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456f7940",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.themes.builder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2cae5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from gradio.themes.base import Base\n",
    "import time\n",
    "\n",
    "class Seafoam(Base):\n",
    "    pass\n",
    "\n",
    "seafoam = Seafoam()\n",
    "\n",
    "with gr.Blocks(theme=seafoam) as demo:\n",
    "    textbox = gr.Textbox(label=\"Name\")\n",
    "    slider = gr.Slider(label=\"Count\", minimum=0, maximum=100, step=1)\n",
    "    with gr.Row():\n",
    "        button = gr.Button(\"Submit\", variant=\"primary\")\n",
    "        clear = gr.Button(\"Clear\")\n",
    "    output = gr.Textbox(label=\"Output\")\n",
    "\n",
    "    def repeat(name, count):\n",
    "        time.sleep(3)\n",
    "        return name * count\n",
    "    \n",
    "    button.click(repeat, [textbox, slider], output)\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cff3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "     def greet(name, intensity):\n",
    "    return \"Hello, \" + name + \"!\" * int(intensity)\n",
    "\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=greet,\n",
    "    inputs=[\"text\", \"slider\"],\n",
    "    outputs=[\"text\"],\n",
    ")\n",
    "\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6865f1ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "title = \"Multiple Interfaces\"\n",
    "\n",
    "#app 1\n",
    "def user_greeting(name):\n",
    "    return \"Hi! \" + name + \" Welcome to your first Gradio application!ðŸ˜Ž\"\n",
    "\n",
    "#app 2\n",
    "def user_help(do):\n",
    "    return \"So today we will do \" + do + \"using Gradio. Great choice!\"\n",
    "\n",
    "#interface 1\n",
    "app1 =  gr.Interface(fn = user_greeting, inputs=\"text\", outputs=\"text\")\n",
    "#interface 2\n",
    "\n",
    "app2 =  gr.Interface(fn = user_help, inputs=\"text\", outputs=\"text\")\n",
    "\n",
    "demo = gr.TabbedInterface([app1, app2], [\"Welcome\", \"What to do\"])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5724fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f95049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_audio(audio):\n",
    "    sr, data = audio\n",
    "    return (sr, np.flipud(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3061c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_audio = gr.Audio(sources=[\"microphone\"])\n",
    "demo = gr.Interface(\n",
    "    fn=reverse_audio,\n",
    "    inputs=input_audio,\n",
    "    outputs=\"audio\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6114a395",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb12549e",
   "metadata": {},
   "source": [
    "Speech to Audio with Gradio ASR and transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8fad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the ASR pipeline\n",
    "transcriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n",
    "\n",
    "def transcribe(audio):\n",
    "    sr, y = audio # Extract sampling rate and audio data\n",
    "    y = y.astype(np.float32) # Convert audio data to float32\n",
    "    y /= np.max(np.abs(y)) # Normalize the audio data\n",
    "    return transcriber({\"sampling_rate\": sr, \"raw\": y})[\"text\"] # Transcribe the audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9608ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = gr.Interface(\n",
    "    transcribe,\n",
    "    gr.Audio(sources=[\"microphone\"]),\n",
    "    \"text\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a78cbf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e99dcf",
   "metadata": {},
   "source": [
    "Gradio Interface with ASR returning text to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2d08f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the ASR pipeline\n",
    "transcriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n",
    "\n",
    "def transcribe(audio):\n",
    "    sr, y = audio\n",
    "    y = y.astype(np.float32)\n",
    "    y /= np.max(np.abs(y))\n",
    "    return transcriber({\"sampling_rate\": sr, \"raw\": y})[\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7901810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe from transcription\n",
    "\n",
    "def transcribe_to_df(audio):\n",
    "    transcribed_text = transcribe(audio)\n",
    "    # Create a DataFrame with the transcribed text\n",
    "    df = pd.DataFrame({\"Transcription\": [transcribed_text]})\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16903d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = gr.Interface(\n",
    "    transcribe_to_df,\n",
    "    gr.Audio(sources=[\"microphone\"]),\n",
    "    outputs=\"dataframe\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64657659",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7724cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_push = pd.DataFrame(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae8e20b",
   "metadata": {},
   "source": [
    "Gradio audio caputre to Assembly AI sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8cd098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Placeholder function to stream audio to AssemblyAI\n",
    "def stream_audio_to_assemblyai(audio_stream):\n",
    "    # This function should establish a WebSocket connection to AssemblyAI,\n",
    "    # send the audio stream in real-time, and return the transcription and sentiment analysis results.\n",
    "    # The actual implementation depends on the AssemblyAI API and WebSocket handling.\n",
    "    pass\n",
    "\n",
    "# Gradio interface for live audio input\n",
    "def process_live_audio(audio_stream):\n",
    "    # Process the live audio stream\n",
    "    transcription_and_sentiment = stream_audio_to_assemblyai(audio_stream)\n",
    "    return transcription_and_sentiment\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=process_live_audio,\n",
    "    inputs=gr.Audio(sources=[\"microphone\"], streaming=True),\n",
    "    outputs=[\"text\", \"json\"], # Assuming sentiment analysis results are returned in JSON format\n",
    "    live=True,\n",
    ")\n",
    "\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495d8c05",
   "metadata": {},
   "source": [
    "Gradio ASR to Assembly AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd57f49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "# Initialize the ASR pipeline\n",
    "transcriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n",
    "\n",
    "def transcribe(audio):\n",
    "    sr, y = audio\n",
    "    y = y.astype(np.float32)\n",
    "    y /= np.max(np.abs(y))\n",
    "    return transcriber({\"sampling_rate\": sr, \"raw\": y})[\"text\"]\n",
    "\n",
    "def send_to_assemblyai(text):\n",
    "    # Replace with your actual AssemblyAI API endpoint and API key\n",
    "    url = \"https://api.assemblyai.com/v2/transcript\"\n",
    "    headers = {\n",
    "        \"Authorization\": \"ASSEMBLY_API_KEY\"\n",
    "    }\n",
    "    data = {\n",
    "        \"text\": text,\n",
    "        # Additional parameters as needed\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return response.json()\n",
    "\n",
    "def process_audio(audio):\n",
    "    transcribed_text = transcribe(audio)\n",
    "    assemblyai_response = send_to_assemblyai(transcribed_text)\n",
    "    return assemblyai_response\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=process_audio,\n",
    "    inputs=gr.Audio(sources=[\"microphone\"]),\n",
    "    outputs=\"json\",\n",
    ")\n",
    "\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fae4146e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import assemblyai as aai\n",
    "\n",
    "aai.settings.api_key = \"ASSEMBLYAI_API_KEY\"\n",
    "transcriber = aai.Transcriber()\n",
    "\n",
    "transcript = transcriber.transcribe(\"https://storage.googleapis.com/aai-web-samples/news.mp4\")\n",
    "# transcript = transcriber.transcribe(\"./my-local-audio-file.wav\")\n",
    "\n",
    "print(transcript.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e56c36b",
   "metadata": {},
   "source": [
    "# Gradio ASR to output txt file (Works!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f80a881d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "Running on public URL: https://cab7c98d5ab65126a9.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://cab7c98d5ab65126a9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the ASR pipeline\n",
    "transcriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n",
    "\n",
    "def transcribe_and_save(audio):\n",
    "    sr, y = audio\n",
    "    y = y.astype(np.float32)\n",
    "    y /= np.max(np.abs(y))\n",
    "    transcription = transcriber({\"sampling_rate\": sr, \"raw\": y})[\"text\"]\n",
    "    \n",
    "    # Write the transcription to a file\n",
    "    with open(\"transcription.txt\", \"w\") as file:\n",
    "        file.write(transcription)\n",
    "    \n",
    "    return transcription\n",
    "\n",
    "# Create the Gradio interface\n",
    "demo = gr.Interface(\n",
    "    transcribe_and_save,\n",
    "    gr.Audio(sources=[\"microphone\"]),\n",
    "    \"text\",\n",
    ")\n",
    "\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7cd700e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment analysis with OpenAI\n",
    "#preprocess txt file\n",
    "\n",
    "with open('transcription.txt', 'r', encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a201c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4-turbo-2024-04-09\n",
      "whisper-1\n",
      "davinci-002\n",
      "gpt-4-turbo\n",
      "gpt-4-1106-preview\n",
      "gpt-4-0613\n",
      "dall-e-2\n",
      "tts-1-hd-1106\n",
      "tts-1-hd\n",
      "gpt-4\n",
      "gpt-4-1106-vision-preview\n",
      "gpt-3.5-turbo-0125\n",
      "gpt-3.5-turbo\n",
      "gpt-3.5-turbo-16k\n",
      "gpt-3.5-turbo-16k-0613\n",
      "tts-1\n",
      "dall-e-3\n",
      "gpt-3.5-turbo-0613\n",
      "gpt-3.5-turbo-0301\n",
      "gpt-3.5-turbo-instruct-0914\n",
      "babbage-002\n",
      "gpt-3.5-turbo-instruct\n",
      "tts-1-1106\n",
      "text-embedding-3-large\n",
      "gpt-4-0125-preview\n",
      "text-embedding-3-small\n",
      "text-embedding-ada-002\n",
      "gpt-3.5-turbo-1106\n",
      "gpt-4-turbo-preview\n",
      "gpt-4-vision-preview\n"
     ]
    }
   ],
   "source": [
    "model_lst = openai.Model.list()\n",
    "\n",
    "for i in model_lst['data']:\n",
    "    print(i['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "be62055e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of the text is I am a girl and I am 15 years old. I am from Russia. I like to read books and watch movies. I like to play the piano and sing. I like to go to the cinema and to the theater. I like to go to the park and to the beach. I like to go to the disco and to the club. I like to go to the restaurant and to the cafe. I like to go to the museum and to the zoo. I like to go to the library and to the bookstore. I like to go to the gym and to the swimming pool. I like to go to the cinema and\n"
     ]
    }
   ],
   "source": [
    "#analysis\n",
    "\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Function to perform sentiment analysis using OpenAI\n",
    "def perform_sentiment_analysis(text):\n",
    "    prompt = f\"Please analyze the sentiment of the following text:\\n{text}\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"davinci-002\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.1,\n",
    "        max_tokens=128,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        timeout=10,\n",
    "    )\n",
    "    sentiment = response.choices[0].text.strip().replace(\"The sentiment of the text is \", \"\").rstrip('.')\n",
    "    return sentiment\n",
    "\n",
    "# Read the text file\n",
    "with open('transcription.txt', 'r', encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Perform sentiment analysis\n",
    "sentiment = perform_sentiment_analysis(text)\n",
    "print(f\"The sentiment of the text is {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff6a1e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93287c47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
